{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GANs.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yandexdataschool/MLatImperial2019/blob/master/06_lab/GANs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "ZKatwHToL7vN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Generative adversarial networks"
      ]
    },
    {
      "metadata": {
        "id": "9sROITncmZ5H",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/yandexdataschool/MLatImperial2019/master/06_lab/lfw_dataset.py"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RY5JMnzgMb4P",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's do our usual imports + use a tool to download the dataset we'll use today:"
      ]
    },
    {
      "metadata": {
        "id": "nereROxJhE4B",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "\n",
        "from lfw_dataset import fetch_lfw_dataset\n",
        "data, attrs = fetch_lfw_dataset(dimx=36, dimy=36)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "b9S6ZuQaMntn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "So far our data has the following shape: (n_images, height, width, n_channels). PyTorch convolutional layers want the channels dimension to be the second one (axis=1), so let's transpose (and normalize) the data:"
      ]
    },
    {
      "metadata": {
        "id": "SZopvYBFmtSl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data = data.transpose(0, 3, 1, 2).astype(np.float32) / 255."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "958mLZ1dNPZD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "And here's a function to plot a (optionally random) subset of images:"
      ]
    },
    {
      "metadata": {
        "id": "zqtnOTerna5R",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def plot_mn(images, m=5, n=5, shuffle=True):\n",
        "  if shuffle:\n",
        "    images = images[np.random.permutation(len(images))[:m * n]]\n",
        "  h, w = images.shape[2:]\n",
        "  images = images[:m*n].reshape(m, n, *images.shape[1:]).transpose(0, 1, 3, 4, 2)\n",
        "  images = images.transpose(0, 2, 1, 3, 4).reshape(m * h, n * w, -1)\n",
        "  plt.imshow(images)\n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "plot_mn(data)\n",
        "plt.axis('off');"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UlB7VAfPNc38",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Our data shape is:"
      ]
    },
    {
      "metadata": {
        "id": "Le0zM3-nidlK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "U8zir29ENgF7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Also, let's mount gdrive to save the models later on:"
      ]
    },
    {
      "metadata": {
        "id": "mp0vQcwjJ22o",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "u9Pg3gYBN05p",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Finally, let's import torch and define the Reshape layer (same as in the introduction to PyTorch):"
      ]
    },
    {
      "metadata": {
        "id": "-Z7YAPGxp2Xl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "class Reshape(torch.nn.Module):\n",
        "  def __init__(self, *shape):\n",
        "    super(Reshape, self).__init__()\n",
        "    self.shape = shape\n",
        "\n",
        "  def forward(self, x):\n",
        "    return x.reshape(*self.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ORVT4PnCN_0h",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now, we'll take off from a very simple generator and discriminator:"
      ]
    },
    {
      "metadata": {
        "id": "2Db-LPTKOIhf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "latent_dims = 128\n",
        "dropout_rate = 0.1\n",
        "\n",
        "generator = torch.nn.Sequential(\n",
        "    torch.nn.Linear(latent_dims, 64),\n",
        "    torch.nn.ELU(),\n",
        "    torch.nn.Dropout(p=dropout_rate),\n",
        "    torch.nn.Linear(64, 3 * 36 * 36),\n",
        "    torch.nn.ELU(),\n",
        "    Reshape(-1, 3, 36, 36)\n",
        ").cuda()\n",
        "\n",
        "discriminator = torch.nn.Sequential(\n",
        "    Reshape(-1, 3 * 36 * 36),\n",
        "    torch.nn.Linear(3 * 36 * 36, 128),\n",
        "    torch.nn.ELU(),\n",
        "    torch.nn.Dropout(p=dropout_rate),\n",
        "    torch.nn.Linear(128, 1),\n",
        ").cuda()\n",
        "\n",
        "def get_n_params(model):\n",
        "  return sum(p.reshape(-1).shape[0] for p in model.parameters())\n",
        "\n",
        "print('generator params:', get_n_params(generator))\n",
        "print('discriminator params:', get_n_params(discriminator))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IOiD7VoLPfIz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Then, we need a function to sample real and fake images:"
      ]
    },
    {
      "metadata": {
        "id": "BKuzv2Ok9vAy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def sample_images(batch_size):\n",
        "  ids = np.random.choice(len(data), size=batch_size)\n",
        "  return torch.tensor(data[ids]).cuda()\n",
        "\n",
        "def sample_fake(batch_size):\n",
        "  noise = torch.randn(batch_size, latent_dims).cuda()\n",
        "  return generator(noise)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FyYWtgV4Pn3G",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's have a look what we can generate before any training:"
      ]
    },
    {
      "metadata": {
        "id": "-l_ju5AeyffR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "generator.eval()\n",
        "imgs = sample_fake(25).cpu().detach().numpy()\n",
        "plt.figure(figsize=(8, 8))\n",
        "plot_mn(imgs.clip(0, 1))\n",
        "plt.axis('off')\n",
        "plt.show();"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FLv6NgnbP2iO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Ok, now that we have our model defined, we need our loss functions. Historically the first loss used in GANs is the cross-entropy that we already used so many times:\n",
        "$$\\mathscr{L}^{\\text{discr}} =\n",
        "-\\text{E}\\left[logD(x_{real})\\right]\n",
        "-\\text{E}\\left[log(1 - D(x_{fake}))\\right] \n",
        "$$\n",
        "\n",
        "And hence for the generator the loss is:\n",
        "\n",
        "$$\\mathscr{L}^{\\text{gen}} =\n",
        "-\\text{E}\\left[logD(x_{fake})\\right]$$\n",
        "\n",
        "Note that here $D(x)$ is the probability the discriminator assigns to $x$ to be from the real dataset, so it's $\\sigma($ `discriminator` $(x))$.\n",
        "\n",
        "Try implementing these loss functions below. Note that $1-\\sigma(x)=\\sigma(-x)$. You should use the `logsigmoid` as a stable realization of $log\\cdot\\sigma(x)$."
      ]
    },
    {
      "metadata": {
        "id": "uVYlJGqlTJLJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "logsigmoid = torch.nn.functional.logsigmoid\n",
        "\n",
        "def generator_loss(fake):\n",
        "  return <YOUR CODE HERE>\n",
        "  \n",
        "def discriminator_loss(real, fake):\n",
        "  return <YOUR CODE HERE>"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YP1jCl68UGs_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's do some more set-up and run the learning process:"
      ]
    },
    {
      "metadata": {
        "id": "GIirtIXRkpuD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "\n",
        "generator_losses = []\n",
        "discriminator_losses = []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VCLeJ9SQToLs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "optimizer_generator = \\\n",
        "    torch.optim.RMSprop(generator.parameters(), lr=0.001)\n",
        "optimizer_discriminator = \\\n",
        "    torch.optim.RMSprop(discriminator.parameters(), lr=0.001)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HRPr9l4tTqIN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from IPython.display import clear_output\n",
        "\n",
        "\n",
        "for i in range(1000):\n",
        "  # Set our models to training mode:\n",
        "  generator.train()\n",
        "  discriminator.train()\n",
        "  \n",
        "  # Several discriminator updates per step:\n",
        "  for j in range(5):\n",
        "    # Sampling reals and fakes\n",
        "    real = sample_images(batch_size)\n",
        "    fake = sample_fake(batch_size)\n",
        "    \n",
        "    # Calculating the loss\n",
        "    loss = discriminator_loss(real, fake)\n",
        "    \n",
        "    # Doing our regular optimization step for the discriminator\n",
        "    discriminator.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer_discriminator.step()\n",
        "\n",
        "  # Remember the value of discriminator loss for plotting\n",
        "  discriminator_losses.append(loss.item())\n",
        "\n",
        "  # Now it's generator's time to learn:\n",
        "  loss = generator_loss(sample_fake(batch_size))\n",
        "  generator_losses.append(loss.item())\n",
        "  generator.zero_grad()\n",
        "  loss.backward()\n",
        "  optimizer_generator.step()\n",
        "\n",
        "    \n",
        "  if i % 20 == 0:\n",
        "    clear_output(wait=True)\n",
        "    plt.plot(generator_losses    , label='generator loss')\n",
        "    plt.plot(discriminator_losses, label='discriminator loss')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "    generator.eval()\n",
        "    imgs = sample_fake(25).cpu().detach().numpy()\n",
        "    plt.figure(figsize=(8, 8))\n",
        "    plot_mn(imgs.clip(0, 1))\n",
        "    plt.axis('off')\n",
        "    plt.show();"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RUWQQK7_VRrR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Seems like it's working! Now try to impelment some convolutional architechtures to make our model more expressive.\n",
        "\n",
        "You might face some instabilities as you go. The simplest regularization that can be done is **adding some random noize to the discriminator's input**.\n",
        "\n",
        "Another good stabilization is adding a **discriminator gradient L2 penalty for real images only** (see this paper: https://arxiv.org/pdf/1801.04406.pdf):\n",
        "\n",
        "```\n",
        "def discriminator_penalty(real, size=gradient_penalty):\n",
        "  scores = discriminator(real)\n",
        "  grad_params = torch.autograd.grad(scores.mean(), discriminator.parameters(),\n",
        "                                    create_graph=True)\n",
        "  penalty = sum((grad**2).sum() for grad in grad_params)\n",
        "  return penalty * size\n",
        "```\n",
        "\n",
        "\n",
        "You can also use **decreasing learning rate**:\n",
        "\n",
        "```\n",
        "# multiply the learning rate by 0.999 each 10 steps:\n",
        "scheduler_d = torch.optim.lr_scheduler.StepLR(optimizer_discriminator, step_size=10, gamma=0.999)\n",
        "scheduler_g = torch.optim.lr_scheduler.StepLR(optimizer_generator    , step_size=10, gamma=0.999)\n",
        "\n",
        "# ...\n",
        "\n",
        "# In the training loop:\n",
        "  scheduler_d.step()\n",
        "  scheduler_g.step()\n",
        "```"
      ]
    },
    {
      "metadata": {
        "id": "gtZ5cUFwTqD2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
