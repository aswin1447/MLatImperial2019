{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/yandexdataschool/MLatImperial2019/blob/master/02_lab/lab2_classification_seminar.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression\n",
    "\n",
    "In this seminar you will implement a logistic regression and train it using stochastic gradient descent modiffications, numpy and your brain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two-dimensional classification problem\n",
    "\n",
    "To make things more intuitive, let's solve a 2D classification problem with syntetic data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets, preprocessing\n",
    "\n",
    "(X, y) = datasets.make_circles(n_samples=1024, shuffle=True, noise=0.2, factor=0.4)\n",
    "ind = np.logical_or(y==1, X[:,1] > X[:,0] - 0.5)\n",
    "X = X[ind,:]\n",
    "m = np.array([[1, 1], [-2, 1]])\n",
    "X = preprocessing.scale(X)\n",
    "y = y[ind]\n",
    "\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Paired)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X:\\n{}\\ny:\\n{}\".format(X[:3],y[:3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your task starts here**\n",
    "\n",
    "Since the problem above isn't linearly separable, we add quadratic features to the classifier.\n",
    "\n",
    "Implement this transformation in the __expand__ function __[1 point]__. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand(X):\n",
    "    \"\"\"\n",
    "    Adds quadratic features. \n",
    "    This function allows your linear model to make non-linear separation.\n",
    "    \n",
    "    For each sample (row in matrix), compute an expanded row:\n",
    "    [feature0, feature1, feature0^2, feature1^2, feature1*feature2, 1]\n",
    "    \n",
    "    :param X: matrix of features, shape [n_samples,2]\n",
    "    :returns: expanded features of shape [n_samples,6]\n",
    "    \"\"\"\n",
    "    X_expanded = np.zeros((X.shape[0], 6))\n",
    "    \n",
    "    <your code here>\n",
    "    \n",
    "    return X_expanded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#simple test on random numbers\n",
    "#[all 8 random numbers are 100% random :P]\n",
    "dummy_X = np.array([\n",
    "        [0,0],\n",
    "        [1,0],\n",
    "        [2.61,-1.28],\n",
    "        [-0.59,2.1]\n",
    "    ])\n",
    "\n",
    "#call your expand function\n",
    "dummy_expanded = expand(dummy_X)\n",
    "\n",
    "#what it should have returned:   x0       x1       x0^2     x1^2     x0*x1    1\n",
    "dummy_expanded_ans = np.array([[ 0.    ,  0.    ,  0.    ,  0.    ,  0.    ,  1.    ],\n",
    "                               [ 1.    ,  0.    ,  1.    ,  0.    ,  0.    ,  1.    ],\n",
    "                               [ 2.61  , -1.28  ,  6.8121,  1.6384, -3.3408,  1.    ],\n",
    "                               [-0.59  ,  2.1   ,  0.3481,  4.41  , -1.239 ,  1.    ]])\n",
    "\n",
    "#tests\n",
    "assert isinstance(dummy_expanded,np.ndarray), \"please make sure you return numpy array\"\n",
    "assert dummy_expanded.shape==dummy_expanded_ans.shape, \"please make sure your shape is correct\"\n",
    "assert np.allclose(dummy_expanded,dummy_expanded_ans,1e-3), \"Something's out of order with features\"\n",
    "\n",
    "print(\"Seems legit!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression\n",
    "Now, let's write function that predicts class given X as in logistic regression.\n",
    "\n",
    "The math should look like this:\n",
    "\n",
    "$$ P(y| \\vec x, \\vec w) = \\sigma(\\vec x \\cdot \\vec w )$$\n",
    "\n",
    "where x represents features, w are weights and $$\\sigma(a) = {1 \\over {1+e^{-a}}}$$\n",
    "\n",
    "We shall omit $ \\vec {arrows} $ in further formulae for simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(X, w):\n",
    "    \"\"\"\n",
    "    Given input features and weights\n",
    "    return predicted probabilities of y==1 given x, P(y=1|x), see description above\n",
    "        \n",
    "    __don't forget to expand X inside classify and other functions__\n",
    "    \n",
    "    :param X: feature matrix X of shape [n_samples,2] (non-exanded)\n",
    "    :param w: weight vector w of shape [6] for each of the expanded features\n",
    "    :returns: an array of predicted probabilities in [0,1] interval.\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    return <your code here>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample usage / test just as the previous one\n",
    "dummy_weights = np.linspace(-1,1,6)\n",
    "\n",
    "dummy_probs = classify(dummy_X,dummy_weights)\n",
    "\n",
    "dummy_answers = np.array([ 0.73105858,  0.450166  ,  0.02020883,  0.59844257])\n",
    "\n",
    "assert isinstance(dummy_probs,np.ndarray), \"please return np.array\"\n",
    "assert dummy_probs.shape == dummy_answers.shape, \"please return an 1-d vector with answers for each object\"\n",
    "assert np.allclose(dummy_probs,dummy_answers,1e-3), \"There's something non-canonic about how probabilties are computed\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss you should try to minimize is the Logistic Loss aka crossentropy aka negative log-likelihood:\n",
    "\n",
    "$$ L =  - {1 \\over N} \\sum_i {y \\cdot log P(y|x,w) + (1-y) \\cdot log (1-P(y|x,w))}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(X, y, w):\n",
    "    \"\"\"\n",
    "    Given feature matrix X [n_samples,2], target vector [n_samples] of +1/0,\n",
    "    and weight vector w [6], compute scalar loss function using formula above.\n",
    "    \"\"\"\n",
    "    return <your code here>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_y = np.array([0,1,0,1])\n",
    "dummy_loss = compute_loss(dummy_X,dummy_y,dummy_weights)\n",
    "\n",
    "assert np.allclose(dummy_loss,0.66131), \"something wrong with loss\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we train our model with gradient descent, we gotta compute gradients.\n",
    "\n",
    "To be specific, we need a derivative of loss function over each weight [6 of them].\n",
    "\n",
    "$$ \\nabla L = {\\partial L \\over \\partial w} = ...$$\n",
    "\n",
    "No, we won't be giving you the exact formula this time. Instead, try figuring out a derivative with pen and paper. \n",
    "\n",
    "As usual, we've made a small test for you, but if you need more, feel free to check your math against finite differences (estimate how L changes if you shift w by $10^-5$ or so)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_grad(X, y, w):\n",
    "    \"\"\"\n",
    "    Given feature matrix X [n_samples,2], target vector [n_samples] of +1/0,\n",
    "    and weight vector w [6], compute vector [6] of derivatives of L over each weights.\n",
    "    \"\"\"\n",
    "    return <your code here>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tests\n",
    "dummy_grads = compute_grad(dummy_X,dummy_y,dummy_weights)\n",
    "\n",
    "#correct answers in canonic form\n",
    "dummy_grads_ans = np.array([-0.06504252, -0.21728448, -0.1379879 , -0.43443953,  0.107504  , -0.05003101])\n",
    "\n",
    "assert isinstance(dummy_grads,np.ndarray)\n",
    "assert dummy_grads.shape == (6,), \"must return a vector of gradients for each weight\"\n",
    "assert len(set(np.round(dummy_grads/dummy_grads_ans,3))), \"gradients are wrong\"\n",
    "assert np.allclose(dummy_grads,dummy_grads_ans,1e-3), \"gradients are off by a coefficient\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's an auxiliary function that visualizes the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "\n",
    "h = 0.01\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "def visualize(X, y, w, history):\n",
    "    plt.figure(figsize=(12,6))\n",
    "    \"\"\"draws classifier prediction with matplotlib magic\"\"\"\n",
    "    Z = classify(np.c_[xx.ravel(), yy.ravel()], w)\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.contourf(xx, yy, Z, alpha=0.8)\n",
    "    plt.colorbar()\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Paired)\n",
    "    plt.xlim(xx.min(), xx.max())\n",
    "    plt.ylim(yy.min(), yy.max())\n",
    "    \n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(history)\n",
    "    plt.grid()\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    ymin, ymax = plt.ylim()\n",
    "    plt.ylim(0, ymax)\n",
    "    display.clear_output(wait=True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize(X,y,dummy_weights,[1,0.5,0.25],)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "In this section, we'll use the functions you wrote to train our classifier using stochastic gradient descent.\n",
    "\n",
    "Try to find an optimal learning rate for gradient descent for the given batch size. \n",
    "\n",
    "**Don't change the batch size!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.array([0,0,0,0,0,1])\n",
    "\n",
    "\n",
    "alpha = <learning rate>\n",
    "\n",
    "n_iter = 50\n",
    "batch_size = 4\n",
    "loss = np.zeros(n_iter)\n",
    "plt.figure(figsize=(12,5))\n",
    "for i in range(n_iter):\n",
    "    ind = np.random.choice(X.shape[0], batch_size)\n",
    "    loss[i] = compute_loss(X, y, w)\n",
    "    visualize(X[ind,:], y[ind], w, loss)\n",
    "    \n",
    "    w = w - alpha * compute_grad(X[ind,:], y[ind], w)\n",
    "\n",
    "visualize(X, y, w, loss)\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, let's see what is happening, when we do not normalise features first.\n",
    "## What do you think will happen?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets, preprocessing\n",
    "\n",
    "(X, y) = datasets.make_circles(n_samples=1024, shuffle=True, noise=0.2, factor=0.4)\n",
    "# And make features super diffrent in scale\n",
    "X[:,0] *= 1000\n",
    "ind = np.logical_or(y==1, X[:,1] > X[:,0] - 0.5)\n",
    "X = X[ind,:]\n",
    "m = np.array([[1, 1], [-2, 1]])\n",
    "# Here I switch off scaling\n",
    "#X = preprocessing.scale(X)\n",
    "y = y[ind]\n",
    "\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Paired)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = 0.01\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, 50), np.arange(y_min, y_max, h))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.array([0,0,0,0,0,1])\n",
    "\n",
    "\n",
    "alpha = <learning rate>\n",
    "\n",
    "n_iter = 50\n",
    "batch_size = 4\n",
    "loss = np.zeros(n_iter)\n",
    "plt.figure(figsize=(12,5))\n",
    "for i in range(n_iter):\n",
    "    ind = np.random.choice(X.shape[0], batch_size)\n",
    "    loss[i] = compute_loss(X, y, w)\n",
    "    visualize(X[ind,:], y[ind], w, loss)\n",
    "    \n",
    "    w = w - alpha * compute_grad(X[ind,:], y[ind], w)\n",
    "\n",
    "visualize(X, y, w, loss)\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of imbalanced datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many ways to account for imbalanced classes. The most popolar are:\n",
    "- Undersampling\n",
    "- Oversampling\n",
    "- Loss weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the latter case, one modifies the loss function to give heigher weights, if the misclassification happened on the underpresented class.\n",
    "\n",
    "$$ L =  - \\frac{{1 \\over N} \\sum_i w_i({y \\cdot log P(y|x,w) + (1-y) \\cdot log (1-P(y|x,w))})}{\\sum_i w_i}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for the oversampling and undersampling there is a dedicated libraries to work with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "from imblearn.pipeline import make_pipeline\n",
    "from imblearn.over_sampling import ADASYN\n",
    "from imblearn.over_sampling import SMOTE, BorderlineSMOTE, SVMSMOTE, SMOTENC\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.base import BaseSampler\n",
    "\n",
    "def create_dataset(n_samples=1000, weights=(0.01, 0.01, 0.98), n_classes=3,\n",
    "                   class_sep=0.8, n_clusters=1):\n",
    "    return make_classification(n_samples=n_samples, n_features=2,\n",
    "                               n_informative=2, n_redundant=0, n_repeated=0,\n",
    "                               n_classes=n_classes,\n",
    "                               n_clusters_per_class=n_clusters,\n",
    "                               weights=list(weights),\n",
    "                               class_sep=class_sep, random_state=0)\n",
    "\n",
    "\n",
    "def plot_resampling(X, y, sampling, ax):\n",
    "    X_res, y_res = sampling.fit_resample(X, y)\n",
    "    ax.scatter(X_res[:, 0], X_res[:, 1], c=y_res, alpha=0.8, edgecolor='k')\n",
    "    # make nice plotting\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.get_xaxis().tick_bottom()\n",
    "    ax.get_yaxis().tick_left()\n",
    "    ax.spines['left'].set_position(('outward', 10))\n",
    "    ax.spines['bottom'].set_position(('outward', 10))\n",
    "    return Counter(y_res)\n",
    "\n",
    "\n",
    "def plot_decision_function(X, y, clf, ax):\n",
    "    plot_step = 0.02\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, plot_step),\n",
    "                         np.arange(y_min, y_max, plot_step))\n",
    "\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    ax.contourf(xx, yy, Z, alpha=0.4)\n",
    "    ax.scatter(X[:, 0], X[:, 1], alpha=0.8, c=y, edgecolor='k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let us see how deicison boundary will change for logistic regression, if we have more data points from the diffrent classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "ax_arr = (ax1, ax2, ax3, ax4)\n",
    "weights_arr = ((0.01, 0.01, 0.98), (0.01, 0.05, 0.94),\n",
    "               (0.2, 0.1, 0.7), (0.33, 0.33, 0.33))\n",
    "for ax, weights in zip(ax_arr, weights_arr):\n",
    "    X, y = create_dataset(n_samples=1000, weights=weights)\n",
    "    clf = LogisticRegression(multi_class='auto', solver='lbfgs').fit(X, y)\n",
    "    plot_decision_function(X, y, clf, ax)\n",
    "    ax.set_title('LogisticRegression with y={}'.format(Counter(y)))\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random over-sampling can be used to repeat some samples and balance the number of samples between the dataset. It can be seen that with this trivial approach the boundary decision is already less biaised toward the majority class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 7))\n",
    "X, y = create_dataset(n_samples=10000, weights=(0.01, 0.05, 0.94))\n",
    "clf = LinearSVC().fit(X, y)\n",
    "plot_decision_function(X, y, clf, ax1)\n",
    "ax1.set_title('LogisticRegression with y={}'.format(Counter(y)) + \":{}\".format(accuracy_score(y, clf.predict(X))))\n",
    "pipe = make_pipeline(RandomOverSampler(random_state=0), LinearSVC())\n",
    "pipe.fit(X, y)\n",
    "plot_decision_function(X, y, pipe, ax2)\n",
    "ax2.set_title('Decision function for RandomOverSampler' + \":{}\".format(accuracy_score(y, pipe.predict(X))))\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More comlex methods are, for examle SMOTE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(20, 6))\n",
    "X, y = create_dataset(n_samples=10000, weights=(0.01, 0.05, 0.94))\n",
    "\n",
    "clf = LogisticRegression(multi_class='auto', solver='lbfgs').fit(X, y)\n",
    "plot_decision_function(X, y, clf, ax1)\n",
    "ax1.set_title('LogisticRegression with y={}'.format(Counter(y)))\n",
    "sampler = SMOTE()\n",
    "clf = make_pipeline(sampler, LinearSVC())\n",
    "clf.fit(X, y)\n",
    "plot_decision_function(X, y, clf, ax2)\n",
    "ax2.set_title('Decision function for {}'.format(sampler.__class__.__name__))\n",
    "sampler = ADASYN()\n",
    "clf = make_pipeline(sampler, LinearSVC())\n",
    "clf.fit(X, y)\n",
    "plot_decision_function(X, y, clf, ax3)\n",
    "ax3.set_title('Decision function for {}'.format(sampler.__class__.__name__))\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "# Bonus quest \n",
    "\n",
    "If you're done and there's still time left, try implementing __momentum SGD__ as described [here](https://distill.pub/2017/momentum/).\n",
    "\n",
    "Find alpha & beta that results in fastest convergence rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.array([0,0,0,0,0,1])\n",
    "z = np.array([0,0,0,0,0,0])\n",
    "\n",
    "alpha = ???\n",
    "beta = ???\n",
    "\n",
    "<YOUR CODE>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
